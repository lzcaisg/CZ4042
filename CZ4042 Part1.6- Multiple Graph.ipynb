{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lzcai/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Values\n",
    "NUM_FEATURES = 36\n",
    "NUM_CLASSES = 6\n",
    "NUM_HIDDEN = 10\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 32\n",
    "SEED = 10\n",
    "BETA = pow(10, -6)\n",
    "np.random.seed(SEED)\n",
    "DROP = True\n",
    "\n",
    "TRAIN_FILE_NAME = 'sat_train.txt'\n",
    "TEST_FILE_NAME = 'sat_test.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "def scale(X, X_min, X_max):\n",
    "    return (X - X_min)/(X_max-X_min)\n",
    "\n",
    "def process_inputs_from_file(fileName): # Read in data\n",
    "    inputs = np.loadtxt(fileName, delimiter=' ')\n",
    "    X, _Y = inputs[:, :NUM_FEATURES], inputs[:, -1].astype(int)\n",
    "    X = scale(X, np.min(X, axis=0), np.max(X, axis=0))\n",
    "    _Y[_Y == 7] = 6 # Actually dont have, just in case have error data\n",
    "\n",
    "    Y = np.zeros((_Y.shape[0], NUM_CLASSES))\n",
    "    Y[np.arange(_Y.shape[0]), _Y - 1] = 1 #one hot matrix\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = process_inputs_from_file(TRAIN_FILE_NAME)\n",
    "testX, testY = process_inputs_from_file(TEST_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of:\n",
      "- Training-set\t\t 4435\n",
      "- Test-set\t\t 2000\n"
     ]
    }
   ],
   "source": [
    "print (\"Size of:\")\n",
    "print(\"- Training-set\\t\\t\",len(trainX))\n",
    "print(\"- Test-set\\t\\t\",len(testX))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 layers in this graph, namely one input-layer(X), one hidden-layer(W, b) and one output-layer(V, c).\n",
    "\n",
    "The output does not scale to [0, 1] and I dont know why. --- Solved 07 Oct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholder Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, NUM_FEATURES], name='x')\n",
    "d = tf.placeholder(tf.float32, [None, NUM_CLASSES], name='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(feature_no, neuron_no, name, logistic = True):\n",
    "    # From eg.5.2\n",
    "    n_in = feature_no\n",
    "    n_out = neuron_no\n",
    "    W_values = np.asarray(np.random.uniform(low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                                            high=np.sqrt(6. / (n_in + n_out)),\n",
    "                                            size=(n_in, n_out)))\n",
    "    if logistic == True:\n",
    "        W_values *= 4\n",
    "    return(tf.Variable(W_values, dtype=tf.float32, name=name))\n",
    "\n",
    "def init_bias(neuron_no, name):\n",
    "    # From eg.5.2\n",
    "    return(tf.Variable(np.zeros(neuron_no), dtype=tf.float32, name=name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cross_entropy(labels, logits):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "\n",
    "def setup_correct_prediction(labels, logits):\n",
    "    return tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Palsed for multiple graph\n",
    "\n",
    "with tf.variable_scope(\"Hidden_layer\"):\n",
    "    W = init_weights(NUM_FEATURES, NUM_HIDDEN, name=\"Weight_1\")\n",
    "    b = init_bias(NUM_HIDDEN, name=\"Bias_1\")\n",
    "    z = tf.matmul(x, W) + b #syn_input_1\n",
    "    h = tf.nn.sigmoid(z) #out_1\n",
    "\n",
    "with tf.variable_scope(\"Output_layer\"):\n",
    "    V = init_weights(NUM_HIDDEN, NUM_CLASSES, name=\"Weight_2\")\n",
    "    c = init_bias(NUM_CLASSES, name=\"Bias_2\" )\n",
    "    u = tf.matmul(h, V) + c #syn_out_2\n",
    "    y = tf.nn.sigmoid(u) #out_2  # Consider to change to sigmoid"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cross_entropy = setup_cross_entropy(labels=d, logits=y)\n",
    "regularization = tf.nn.l2_loss(V) + tf.nn.l2_loss(W) \n",
    "J = tf.reduce_mean(cross_entropy + BETA * regularization)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "correct_prediction = setup_correct_prediction(labels=d, logits=y)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "train_op = optimizer.minimize(J)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Exist\n"
     ]
    }
   ],
   "source": [
    "# save_dir = \"Z:\\Github\\CZ4042\\save\"\n",
    "save_dir = \"/Users/lzcai/CZ4042 Project/CZ4042/save-mac\"\n",
    "if not os.path.exists(save_dir):\n",
    "    print(\"Not Exist\")\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_str = \"-Drop\" if DROP else \"-Not_Drop\"\n",
    "\n",
    "# save_path = os.path.join(save_dir, str(EPOCHS)+ '-'+ str(BATCH_SIZE)+'-sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/lzcai/CZ4042 Project/CZ4042/save-mac/5000-32-sigmoid'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create TensorFlow Session"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "session = tf.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_variables():\n",
    "    session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "init_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to perform optimization iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to generate next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function to plot train accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train(EPOCHS, BATCH_SIZE, train_acc_record, error=False):\n",
    "    if error:\n",
    "        train_acc_record = [1-tmp for tmp in train_acc_record]\n",
    "        yLabel = \"Train Error\"\n",
    "    else:\n",
    "        yLabel = 'Train Accuracy'\n",
    "        \n",
    "    plt.figure(1)\n",
    "    plt.plot(range(EPOCHS), train_acc_record)\n",
    "    plt.xlabel(str(EPOCHS) + ' iterations')\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.savefig(\"PartA-Train\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".png\")\n",
    "    plt.show()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_test(EPOCHS, BATCH_SIZE, test_acc_record, error=False):\n",
    "    if error:\n",
    "        test_acc_record = [1-tmp for tmp in test_acc_record]\n",
    "        yLabel = \"Test Error\"\n",
    "    else:\n",
    "        yLabel = 'Test Accuracy'\n",
    "    \n",
    "    plt.figure(1)\n",
    "    plt.plot(range(EPOCHS), test_acc_record)\n",
    "    plt.xlabel(str(EPOCHS) + ' iterations')\n",
    "    plt.ylabel('Test accuracy')\n",
    "    plt.savefig(\"PartA-Test\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== For test after training only- =========== #\n",
    "# plot_train(EPOCHS, BATCH_SIZE, train_acc_record, test = True)\n",
    "# plot_test(test_count, BATCH_SIZE, test_acc_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper-function of validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(testX, testY):\n",
    "    output_2_, accuracy_ = session.run([y, accuracy], feed_dict={x: testX, d: testY})\n",
    "    print(output_2_, '\\n',accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RUN!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_backup = []\n",
    "test_acc_backup = []\n",
    "time_usage_backup = []\n",
    "total_time_backup = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_num_list = [5, 10, 15, 20, 25]\n",
    "graph_list = [None for i in range(len(hidden_list))]\n",
    "\n",
    "for i in range(len(hidden_list)):\n",
    "    graph_list[i] = tf.Graph()\n",
    "    with graph_list[i].as_default():\n",
    "        \n",
    "        with tf.variable_scope(\"Hidden_layer\"):\n",
    "            W = init_weights(NUM_FEATURES, hidden_num_list[i], name=\"Weight_1\")\n",
    "            b = init_bias(NUM_HIDDEN, name=\"Bias_1\")\n",
    "            z = tf.matmul(x, W) + b #syn_input_1\n",
    "            h = tf.nn.sigmoid(z) #out_1\n",
    "\n",
    "        with tf.variable_scope(\"Output_layer\"):\n",
    "            V = init_weights(hidden_num_list[i], NUM_CLASSES, name=\"Weight_2\")\n",
    "            c = init_bias(NUM_CLASSES, name=\"Bias_2\" )\n",
    "            u = tf.matmul(h, V) + c #syn_out_2\n",
    "            y = tf.nn.sigmoid(u) #out_2  # Consider to change to sigmoid\n",
    "            \n",
    "        cross_entropy = setup_cross_entropy(labels=d, logits=y)\n",
    "        regularization = tf.nn.l2_loss(V) + tf.nn.l2_loss(W) \n",
    "        J = tf.reduce_mean(cross_entropy + BETA * regularization)\n",
    "        correct_prediction = setup_correct_prediction(labels=d, logits=y)\n",
    "        accuracy = tf.reduce_mean(correct_prediction)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "        train_op = optimizer.minimize(J)\n",
    "        \n",
    "        session = tf.Session()\n",
    "        init_variables()\n",
    "        train_acc = []\n",
    "        save_path = os.path.join(save_dir, str(EPOCHS)+ '-'+ str(BATCH_SIZE)+'-sigmoid')\n",
    "\n",
    "        with session.as_default():\n",
    "            # Ensure we update the global variables rather than local copies.\n",
    "\n",
    "            # Start-time used for printing time-usage below.\n",
    "            start_time = time.time()\n",
    "            train_acc_record = []\n",
    "            test_acc_record = []\n",
    "            epoch_time_record = []\n",
    "\n",
    "            best_test_acc = 0.0\n",
    "            last_improvement = 0\n",
    "            improved_str = \"\"\n",
    "            test_count = 0\n",
    "\n",
    "            mul = int(len(trainX)/BATCH_SIZE)\n",
    "            for i in range(EPOCHS):\n",
    "\n",
    "                epoch_start_time = time.time()\n",
    "                for j in range(mul):\n",
    "                    x_batch, d_batch = next_batch(BATCH_SIZE, trainX, trainY)\n",
    "                    feed_dict_train = {x: x_batch, d: d_batch}\n",
    "                    session.run(train_op, feed_dict=feed_dict_train)\n",
    "\n",
    "                train_acc_record.append(accuracy.eval(feed_dict=feed_dict_train))\n",
    "                epoch_end_time = time.time()\n",
    "                epoch_time_diff = epoch_end_time-epoch_start_time\n",
    "                epoch_time_record.append(epoch_time_diff)\n",
    "\n",
    "                if i % 100 == 0 or i == (EPOCHS - 1):\n",
    "                    test_count += 1\n",
    "                    test_accuracy = session.run(accuracy, feed_dict={x: testX, d: testY})\n",
    "                    test_acc_record.append(test_accuracy)\n",
    "                    if DROP:\n",
    "                        if test_accuracy > best_test_acc:\n",
    "                            best_test_acc = test_accuracy\n",
    "                            last_improvement = i\n",
    "                            saver.save(sess=session, save_path=save_path)\n",
    "                            improved_str = \"*\"\n",
    "                        else:\n",
    "                            improved_str = ''\n",
    "                    else:\n",
    "                        saver.save(sess=session, save_path=save_path)\n",
    "\n",
    "                    print('iter %d: Train accuracy %g'%(i, train_acc_record[i]), 'Test accuracy: ',test_accuracy, improved_str)\n",
    "\n",
    "\n",
    "\n",
    "        # Ending time.\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Difference between start and end-times.\n",
    "        time_dif = end_time - start_time\n",
    "\n",
    "        # Print the time-usage.\n",
    "        print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "        plot_train(EPOCHS, BATCH_SIZE, train_acc_record, error = True)\n",
    "        plot_test(test_count, BATCH_SIZE, test_acc_record)\n",
    "\n",
    "        train_acc_backup.append(train_acc_record)\n",
    "        test_acc_backup.append(test_acc_record)\n",
    "        time_usage_backup.append(epoch_time_record)\n",
    "        total_time_backup.append(time_dif)\n",
    "\n",
    "        #=========== Save all the data for EACH TRAINING Has Done ============#\n",
    "        train_acc_filename = \"PartA-Train_Acc-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "        with open(train_acc_filename, 'wb') as fp:\n",
    "            pickle.dump(train_acc_backup, fp)\n",
    "\n",
    "        test_acc_filename = \"PartA-Test_Acc-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "        with open(train_acc_filename, 'wb') as fp:\n",
    "            pickle.dump(test_acc_backup, fp)\n",
    "\n",
    "        time_usage_filename = \"PartA-Time_Usage-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "        with open(time_usage_filename, 'wb') as fp:\n",
    "            pickle.dump(time_usage_backup, fp)\n",
    "\n",
    "        time_usage_filename = \"PartA-Time_Usage-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "        with open(time_usage_filename, 'wb') as fp:\n",
    "            pickle.dump(time_usage_backup, fp)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# for BATCH_SIZE in [64, 32, 16, 8, 4]:\n",
    "for BATCH_SIZE in [32, 16]:\n",
    "# Counter for total number of iterations performed so far.\n",
    "    init_variables()\n",
    "    print(\"BATCH_SIZE=\", BATCH_SIZE)\n",
    "    total_iterations = 0\n",
    "    train_acc = []\n",
    "    save_path = os.path.join(save_dir, str(EPOCHS)+ '-'+ str(BATCH_SIZE)+'-sigmoid')\n",
    "    \n",
    "    with session.as_default():\n",
    "        # Ensure we update the global variables rather than local copies.\n",
    "\n",
    "        # Start-time used for printing time-usage below.\n",
    "        start_time = time.time()\n",
    "        train_acc_record = []\n",
    "        test_acc_record = []\n",
    "        epoch_time_record = []\n",
    "        \n",
    "        best_test_acc = 0.0\n",
    "        last_improvement = 0\n",
    "        improved_str = \"\"\n",
    "        test_count = 0\n",
    "        \n",
    "        mul = int(len(trainX)/BATCH_SIZE)\n",
    "        for i in range(EPOCHS):\n",
    "            \n",
    "            epoch_start_time = time.time()\n",
    "            for j in range(mul):\n",
    "                x_batch, d_batch = next_batch(BATCH_SIZE, trainX, trainY)\n",
    "                feed_dict_train = {x: x_batch, d: d_batch}\n",
    "                session.run(train_op, feed_dict=feed_dict_train)\n",
    "\n",
    "            train_acc_record.append(accuracy.eval(feed_dict=feed_dict_train))\n",
    "            epoch_end_time = time.time()\n",
    "            epoch_time_diff = epoch_end_time-epoch_start_time\n",
    "            epoch_time_record.append(epoch_time_diff)\n",
    "\n",
    "            if i % 100 == 0 or i == (EPOCHS - 1):\n",
    "                test_count += 1\n",
    "                test_accuracy = session.run(accuracy, feed_dict={x: testX, d: testY})\n",
    "                test_acc_record.append(test_accuracy)\n",
    "                if DROP:\n",
    "                    if test_accuracy > best_test_acc:\n",
    "                        best_test_acc = test_accuracy\n",
    "                        last_improvement = i\n",
    "                        saver.save(sess=session, save_path=save_path)\n",
    "                        improved_str = \"*\"\n",
    "                    else:\n",
    "                        improved_str = ''\n",
    "                else:\n",
    "                    saver.save(sess=session, save_path=save_path)\n",
    "\n",
    "                print('iter %d: Train accuracy %g'%(i, train_acc_record[i]), 'Test accuracy: ',test_accuracy, improved_str)\n",
    "\n",
    "\n",
    "\n",
    "    # Ending time.\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Difference between start and end-times.\n",
    "    time_dif = end_time - start_time\n",
    "\n",
    "    # Print the time-usage.\n",
    "    print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "    plot_train(EPOCHS, BATCH_SIZE, train_acc_record, error = True)\n",
    "    plot_test(test_count, BATCH_SIZE, test_acc_record)\n",
    "\n",
    "    train_acc_backup.append(train_acc_record)\n",
    "    test_acc_backup.append(test_acc_record)\n",
    "    time_usage_backup.append(epoch_time_record)\n",
    "    total_time_backup.append(time_dif)\n",
    "            \n",
    "    #=========== Save all the data for EACH TRAINING Has Done ============#\n",
    "    train_acc_filename = \"PartA-Train_Acc-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "    with open(train_acc_filename, 'wb') as fp:\n",
    "        pickle.dump(train_acc_backup, fp)\n",
    "\n",
    "    test_acc_filename = \"PartA-Test_Acc-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "    with open(train_acc_filename, 'wb') as fp:\n",
    "        pickle.dump(test_acc_backup, fp)\n",
    "\n",
    "    time_usage_filename = \"PartA-Time_Usage-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "    with open(time_usage_filename, 'wb') as fp:\n",
    "        pickle.dump(time_usage_backup, fp)\n",
    "\n",
    "    time_usage_filename = \"PartA-Time_Usage-\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "    with open(time_usage_filename, 'wb') as fp:\n",
    "        pickle.dump(time_usage_backup, fp)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal **number of hidden neurons** for the 3-layer network designed in part(2):\n",
    "\n",
    "- Plot the **training errors** and **test accuracies** against the **number of epochs** for 3-layer network at hidden layer neurons. Limit the search space to the number of hidden neurons to *S = {5 ,10, 15, 20, 25}*\n",
    "\n",
    "- Plot the **time to train** the network for **one epoch** for different number of hidden neurons/\n",
    "\n",
    "- State the rationale for selecting the optimal number of hidden neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32 # Set to the optimal batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the optimal **decay parameter** for the 3-layer network designed with optimal hidden neurons in part (3).\n",
    "\n",
    "- Plot the **training errors** against the **number of epochs** for the 3-layer network for different values of **decay parameters** in search space *S = {0, 10^-3, 10^-6, 10^-9, 10^-12}*\n",
    "\n",
    "- Plot the test accuracies against the different values of decay parameter.\n",
    "\n",
    "- State the rationale for selecting the optimal decay parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you are done with the 3-layer network, **design a 4-layer network** with **two hidden-layers**, each consisting of **10 perceptrons**, trained with a **batch size of 32** and **decay parameter 10^-6**.\n",
    "\n",
    "- Plot the **train and test accuracy** of the 4-layer network.\n",
    "\n",
    "- Compare and comment on the performances on 3-layer and 4-layer networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6, 0.6, 0.6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = [0.4, 0.4, 0.4]\n",
    "tmp = [1-a for a in tmp]\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6, 0.6, 0.6]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
