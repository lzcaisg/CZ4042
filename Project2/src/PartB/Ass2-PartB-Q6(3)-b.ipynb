{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lzcai/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = 128\n",
    "MAX_DOC_LEN = 100\n",
    "CHAR_DEPTH = 256\n",
    "WORD_WIDTH = 20\n",
    "NUM_CLASSES = 15\n",
    "DROP = True\n",
    "EMBEDDING_SIZE = 50\n",
    "NUM_HIDDEN = 20\n",
    "NUM_LAYER = 2\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "SAVE_DIR = \"../data/PartB_Result\"\n",
    "RESULT_DIR = \"../data/PartB_Result/Q6/3-b\"\n",
    "TRAIN_CHAR_FILENAME = \"Train_char.out\"\n",
    "TEST_CHAR_FILENAME = \"Test_char.out\"\n",
    "TRAIN_WORD_FILENAME = \"Train_word_raw.out\"\n",
    "TEST_WORD_FILENAME = \"Test_word_raw.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_DIR, TRAIN_CHAR_FILENAME),\"rb\") as f:\n",
    "    trainX, tmp_trainY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_DIR, TEST_CHAR_FILENAME),\"rb\") as f:\n",
    "    testX, tmp_testY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_targets = np.array(tmp_trainY).reshape(-1)\n",
    "trainY = np.eye(NUM_CLASSES)[trainY_targets]\n",
    "\n",
    "testY_targets = np.array(tmp_testY).reshape(-1)\n",
    "testY = np.eye(NUM_CLASSES)[testY_targets]\n",
    "# trainY = y_train\n",
    "# testY = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_a_cell(size):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(size)\n",
    "#     drop = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return cell\n",
    "\n",
    "def rnn_model(x, n_words):\n",
    "\n",
    "    word_vectors = tf.contrib.layers.embed_sequence(\n",
    "        x, vocab_size=n_words, embed_dim=EMBEDDING_SIZE)\n",
    "\n",
    "    word_list = tf.unstack(word_vectors, axis=1)\n",
    "\n",
    "#     cell = tf.nn.rnn_cell.GRUCell(NUM_HIDDEN)\n",
    "    cells = [get_a_cell(NUM_HIDDEN) for _ in range(NUM_LAYER)]\n",
    "    stacked_rnn_cell = tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "    print(\"stacked_rnn_cell: \", stacked_rnn_cell)\n",
    "    cell_wrapped = tf.contrib.rnn.OutputProjectionWrapper(stacked_rnn_cell, output_size = 1)\n",
    "    print(\"cell_wrapped: \", cell_wrapped)\n",
    "    \n",
    "    _, encoding = tf.nn.static_rnn(cell_wrapped, word_list, dtype=tf.float32)\n",
    "    print(\"encoding: \", encoding)\n",
    "    \n",
    "    logits = tf.layers.dense(encoding[1], NUM_CLASSES, activation=None)\n",
    "\n",
    "    return logits, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_chars = max([max(tmp) for tmp in trainX]+[max(tmp) for tmp in testX])+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min([min(tmp) for tmp in trainX]+[min(tmp) for tmp in testX])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cross_entropy(labels, logits):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "\n",
    "def setup_correct_prediction(labels, logits):\n",
    "    return tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stacked_rnn_cell:  <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x1c365faa20>\n",
      "cell_wrapped:  <tensorflow.contrib.rnn.python.ops.core_rnn_cell.OutputProjectionWrapper object at 0x1c365fa208>\n",
      "encoding:  (<tf.Tensor 'rnn/rnn/output_projection_wrapper/output_projection_wrapper/multi_rnn_cell/cell_0/gru_cell/add_99:0' shape=(?, 20) dtype=float32>, <tf.Tensor 'rnn/rnn/output_projection_wrapper/output_projection_wrapper/multi_rnn_cell/cell_1/gru_cell/add_99:0' shape=(?, 20) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int64, [None, MAX_DOC_LEN])\n",
    "d = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "y, word_list = rnn_model(x, n_chars)\n",
    "\n",
    "cross_entropy = setup_cross_entropy(labels=d, logits=y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = setup_correct_prediction(labels=d, logits=y)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    print(\"Not Exist\")\n",
    "    os.makedirs(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(EPOCHS, BATCH_SIZE, acc_record, fileName, isTrain, error = False):\n",
    "    if error:  \n",
    "        acc_record = [1-tmp for tmp in acc_record]\n",
    "        if isTrain:\n",
    "            yLabel = 'Train error'\n",
    "        else:\n",
    "            yLabel = 'Test error'\n",
    "    else:\n",
    "        if isTrain:\n",
    "            yLabel = 'Train accuracy'\n",
    "        else:\n",
    "            yLabel = 'Test accuracy'\n",
    "    plt.figure(1)\n",
    "    plt.plot(range(EPOCHS), acc_record)\n",
    "    plt.xlabel(str(EPOCHS) + ' iterations')\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.savefig(os.path.join(RESULT_DIR, fileName))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(testX, testY):\n",
    "    output_2_, accuracy_ = session.run([y, accuracy], feed_dict={x: testX, d: testY})\n",
    "    print(output_2_, '\\n',accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_backup = []\n",
    "test_acc_backup = []\n",
    "time_usage_backup = []\n",
    "total_time_backup = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0: Train accuracy 0.0783929 Test accuracy:  0.07857143 *\n",
      "Time usage: 0:00:53\n"
     ]
    }
   ],
   "source": [
    "# print(\"BATCH_SIZE=\", BATCH_SIZE)\n",
    "total_iterations = 0\n",
    "train_acc = []\n",
    "start_time = time.time()\n",
    "train_acc_record = []\n",
    "test_acc_record = []\n",
    "epoch_time_record = []\n",
    "\n",
    "best_test_acc = 0.0\n",
    "last_improvement = 0\n",
    "improved_str = \"\"\n",
    "test_count = 0\n",
    "train_count = 0\n",
    "mul = int(len(trainX)/BATCH_SIZE)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "#         for j in range(len(x_allBatch)):\n",
    "#         for j in range(mul):\n",
    "#             feed_dict_train = {x: x_allBatch[j], d: y_allBatch[j]}\n",
    "#             x_batch, d_batch = next_batch(BATCH_SIZE, trainX, trainY)\n",
    "        feed_dict_train = {x: trainX, d: trainY}\n",
    "        sess.run(train_op, feed_dict=feed_dict_train)\n",
    "        train_acc_record.append(accuracy.eval(feed_dict=feed_dict_train))\n",
    "        train_count += 1\n",
    "            \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time_diff = epoch_end_time-epoch_start_time\n",
    "        epoch_time_record.append(epoch_time_diff)\n",
    "\n",
    "        if (i % 100 == 0) or (i == (EPOCHS - 1)):\n",
    "            test_count += 1\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x: testX, d: testY})\n",
    "            test_acc_record.append(test_accuracy)\n",
    "            if DROP:\n",
    "                if test_accuracy > best_test_acc:\n",
    "                    best_test_acc = test_accuracy\n",
    "                    last_improvement = i\n",
    "                    saver.save(sess=sess, save_path=RESULT_DIR)\n",
    "                    improved_str = \"*\"\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "            else:\n",
    "                saver.save(sess=sess, save_path=RESULT_DIR)\n",
    "\n",
    "            print('iter %d: Train accuracy %g'%(i, train_acc_record[i]), 'Test accuracy: ',test_accuracy, improved_str)\n",
    "            print(\"Time usage: \" + str(timedelta(seconds=int(round(time.time()-start_time)))))\n",
    "\n",
    "\n",
    "# Ending time.\n",
    "end_time = time.time()\n",
    "\n",
    "# Difference between start and end-times.\n",
    "time_dif = end_time - start_time\n",
    "\n",
    "# Print the time-usage.\n",
    "print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "train_graphName = \"PartB-Q6-Train\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".png\"\n",
    "test_graphName = \"PartB-Q6-Test\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".png\"\n",
    "plot_graph(train_count, BATCH_SIZE, train_acc_record,train_graphName, isTrain = True, error=True)\n",
    "plot_graph(test_count, BATCH_SIZE, test_acc_record, test_graphName, isTrain = False)\n",
    "\n",
    "\n",
    "train_acc_backup.append(train_acc_record)\n",
    "test_acc_backup.append(test_acc_record)\n",
    "time_usage_backup.append(epoch_time_record)\n",
    "total_time_backup.append(time_dif)\n",
    "\n",
    "#=========== Save all the data for EACH TRAINING Has Done ============#\n",
    "fileNameTail = str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "\n",
    "train_acc_filename = \"PartB-Q6-Train_Acc-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, train_acc_filename), 'wb') as fp:\n",
    "    pickle.dump(train_acc_backup, fp)\n",
    "\n",
    "test_acc_filename = \"PartB-Q6-Test_Acc-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, train_acc_filename), 'wb') as fp:\n",
    "    pickle.dump(test_acc_backup, fp)\n",
    "\n",
    "time_usage_filename = \"PartB-Q6-Time_Usage-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, time_usage_filename), 'wb') as fp:\n",
    "    pickle.dump(time_usage_backup, fp)\n",
    "\n",
    "time_usage_filename = \"PartB-Q6-Time_Usage-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, time_usage_filename), 'wb') as fp:\n",
    "    pickle.dump(time_usage_backup, fp)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_allBatch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def generate_batch(X_in, y_in, batch_size):\n",
    "    X = list(X_in)\n",
    "    y = list(y_in)\n",
    "    \n",
    "    if len(X)!=len(y):\n",
    "        print(\"Error: len(X)!=len(Y)\")\n",
    "        return None\n",
    "    batched_X = []\n",
    "    batched_y = []\n",
    "    count = 0\n",
    "    while (len(X) >= batch_size):\n",
    "        batched_X.append(X[0:batch_size])\n",
    "        del X[0:batch_size]\n",
    "        batched_y.append(y[0:batch_size])\n",
    "        del y[0:batch_size]\n",
    "        if count % 10 == 0:\n",
    "            print (count)\n",
    "        count += 1\n",
    "    \n",
    "    if len(X) != 0:\n",
    "        remain = batch_size-len(X)\n",
    "        X.extend(batched_X[0][0:remain])\n",
    "        y.extend(batched_y[0][0:remain])\n",
    "        batched_X.append(X)\n",
    "        batched_y.append(y)\n",
    "        print(count, \"Remain rescaled to\", len(X))\n",
    "    \n",
    "    return (batched_X, batched_y)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_allBatch, y_allBatch = generate_batch(trainX, trainY, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_allBatch = np.array(x_allBatch)\n",
    "y_allBatch = np.array(y_allBatch)\n",
    "\n",
    "print(x_allBatch.shape)\n",
    "print(y_allBatch.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def data_read_words():\n",
    "  \n",
    "    x_train, y_train, x_test, y_test = [], [], [], []\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, 'train_medium.csv'), encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            x_train.append(row[2])\n",
    "            y_train.append(int(row[0]))\n",
    "\n",
    "    with open(os.path.join(DATA_DIR, \"test_medium.csv\"), encoding='utf-8') as filex:\n",
    "        reader = csv.reader(filex)\n",
    "        for row in reader:\n",
    "            x_test.append(row[2])\n",
    "            y_test.append(int(row[0]))\n",
    "\n",
    "    x_train = pandas.Series(x_train)\n",
    "    y_train = pandas.Series(y_train)\n",
    "    x_test = pandas.Series(x_test)\n",
    "    y_test = pandas.Series(y_test)\n",
    "    y_train = y_train.values\n",
    "    y_test = y_test.values\n",
    "\n",
    "    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(\n",
    "        MAX_DOC_LEN)\n",
    "\n",
    "    x_transform_train = vocab_processor.fit_transform(x_train)\n",
    "    x_transform_test = vocab_processor.transform(x_test)\n",
    "\n",
    "    x_train = np.array(list(x_transform_train))\n",
    "    x_test = np.array(list(x_transform_test))\n",
    "\n",
    "    no_words = len(vocab_processor.vocabulary_)\n",
    "    print('Total words: %d' % no_words)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, no_words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "x_train, y_train, x_test, y_test, n_words = data_read_words()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
