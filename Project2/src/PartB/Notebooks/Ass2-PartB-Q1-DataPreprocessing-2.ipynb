{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lzcai/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta\n",
    "import gensim\n",
    "import sklearn.decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 128\n",
    "MAX_DOC_LEN=100\n",
    "CHAR_NUM=256\n",
    "WORD_WIDTH = 20\n",
    "W2V_WIDTH = 300\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data\"\n",
    "SAVE_DIR = \"../data/PartB_Result\"\n",
    "TRAIN_FILENAME = \"train_medium.csv\"\n",
    "TEST_FILENAME = \"test_medium.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2words(text):\n",
    "    words = [tmp.split(' ')[1:] for tmp in text]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2flatten_feature(X_words):\n",
    "    # words = X_train_words\n",
    "    feature = []\n",
    "    hash_feature = []\n",
    "    for words in X_words:\n",
    "        tmp_hash = []\n",
    "        for tmp in words:\n",
    "            try:\n",
    "                feature.append(model.get_vector(tmp))\n",
    "                tmp_hash.append(1)\n",
    "            except KeyError as err:\n",
    "#                 print(err)\n",
    "                tmp_hash.append(0)\n",
    "                pass\n",
    "        hash_feature.append(tmp_hash)\n",
    "    print(len(feature))\n",
    "    return [feature, hash_feature]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_structured_feat(feature, hash_feature):\n",
    "    count = 0\n",
    "    feature_list = []\n",
    "    for i in range(len(hash_feature)):\n",
    "        tmp_feat_list = []\n",
    "        for j in range(len(hash_feature[i])):\n",
    "            if hash_feature[i][j]:\n",
    "\n",
    "                tmp_feat_list.append(feature[count])\n",
    "                count += 1\n",
    "            else:\n",
    "                tmp_feat_list.append(np.zeros((WORD_WIDTH,), dtype = np.float32))\n",
    "        feature_list.append(tmp_feat_list)\n",
    "    \n",
    "    print(count)\n",
    "    return feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_structure(feat_structured):\n",
    "    for i, element in enumerate(feat_structured):\n",
    "        if(len(element)>=MAX_DOC_LEN):\n",
    "            feat_structured[i]=element[:MAX_DOC_LEN]\n",
    "        else:\n",
    "            remain_num = MAX_DOC_LEN-len(element)\n",
    "            for j in range(remain_num):\n",
    "                feat_structured[i].append(np.zeros((WORD_WIDTH,), dtype=np.float32))\n",
    "    return feat_structured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Volumes/Transcend/Google Word2Vec/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8', 'Sampara River', ' Sampara River is a river in Sulawesi Indonesia.']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_DIR, TEST_FILENAME),'r', encoding='ISO-8859-1', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    test_list = list(reader)\n",
    "    print(test_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['7', 'Park Dinor', ' Park Dinor is a historic diner located at Lawrence Park Township Erie County Pennsylvania. It was built in 1948 by the Paterson Vehicle Co. of Paterson New Jersey. It is a one-story pre-fabricated stainless steel and porcelain building. The diner measures 40 feet long by 14 feet wide and has an attached concrete block kitchen measuring 13 feet by 11 feet six inches. Also on the property is a contributing pre-fabricated 384 square foot cottage also erected in 1948.']\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(DATA_DIR, TRAIN_FILENAME),'r', encoding='ISO-8859-1', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    train_list = list(reader)\n",
    "    print(train_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(train_list, test_list):\n",
    "    y_test =[int(tmp[0]) for tmp in test_list]\n",
    "    y_train =[int(tmp[0]) for tmp in train_list]\n",
    "    X_train_text = [tmp[2] for tmp in train_list]\n",
    "    X_test_text = [tmp[2] for tmp in test_list]\n",
    "    print(\"1/7 :Get Text Finished!\")\n",
    "    \n",
    "    X_train_words = text2words(X_train_text)\n",
    "    X_test_words = text2words(X_test_text)\n",
    "    del X_train_text, X_test_text\n",
    "    print(\"2/7 :Get Words Finished!\")\n",
    "    \n",
    "    X_train_flatten_feat, X_train_hash = word2flatten_feature(X_train_words)\n",
    "    X_test_flatten_feat, X_test_hash = word2flatten_feature(X_test_words)\n",
    "    del X_train_words, X_test_words\n",
    "    print(\"3/7 :Get Flattened Feature Finished!\")\n",
    "    \n",
    "    pca = sklearn.decomposition.PCA(n_components=20)\n",
    "    pca.fit(X_train_flatten_feat+X_test_flatten_feat)\n",
    "    print(\"4/7 :PCA Set-up Finished!\")\n",
    "    \n",
    "    X_train_flatten_feat_20 =  pca.transform(X_train_flatten_feat)\n",
    "    X_test_flatten_feat_20 = pca.transform(X_test_flatten_feat)\n",
    "    del X_train_flatten_feat, X_test_flatten_feat, pca\n",
    "    print(\"5/7 :Feature Projection Finished!\")\n",
    "    \n",
    "    X_train_structured_feat = generate_structured_feat(X_train_flatten_feat_20, X_train_hash)\n",
    "    X_test_structured_feat = generate_structured_feat(X_test_flatten_feat_20, X_test_hash)\n",
    "    del X_train_flatten_feat_20, X_train_hash, X_test_flatten_feat_20, X_test_hash\n",
    "    print(\"6/7 :Structure Feature Finished!\")\n",
    "    \n",
    "    X_train_structured_feat_padded = pad_structure(X_train_structured_feat)\n",
    "    X_test_structured_feat_padded = pad_structure(X_test_structured_feat)\n",
    "    del X_train_structured_feat, X_test_structured_feat\n",
    "    print(\"7/7 :Convert to Fixed-length Finished!\")\n",
    "    \n",
    "    return [np.array(X_train_structured_feat_padded), np.array(X_test_structured_feat_padded),\n",
    "           np.array(y_train), np.array(y_test)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/7 :Get Text Finished!\n",
      "2/7 :Get Words Finished!\n",
      "190510\n",
      "24222\n",
      "3/7 :Get Flattened Feature Finished!\n",
      "4/7 :PCA Set-up Finished!\n",
      "5/7 :Feature Projection Finished!\n",
      "190510\n",
      "24222\n",
      "6/7 :Structure Feature Finished!\n",
      "7/7 :Convert to Fixed-length Finished!\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = process_data(train_list, test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5600, 100, 20)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 100, 20)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.39491441e-01, -2.39416296e-01, -5.17518519e-01, ...,\n",
       "         1.36470550e-01,  9.99214917e-02,  2.42152384e-01],\n",
       "       [ 3.74748950e-01,  6.41365719e-02,  6.38051664e-04, ...,\n",
       "         2.39254077e-02, -2.22814218e-02,  5.21283842e-02],\n",
       "       [-1.02763526e+00,  1.11364808e+00, -2.32645456e-01, ...,\n",
       "         1.69579687e-02, -3.32982390e-02, -1.36554340e-02],\n",
       "       ...,\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_DIR, \"Train_words_20.out\"), 'wb') as fp:\n",
    "    pickle.dump([X_train, y_train], fp)\n",
    "with open(os.path.join(SAVE_DIR, \"Test_words_20.out\"), 'wb') as fp:\n",
    "    pickle.dump([X_test, y_test], fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
