{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For read in model and alter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzcai\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pylab as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data\n",
    "def scale(X, X_min, X_max):\n",
    "    return (X - X_min)/(X_max-X_min)\n",
    "\n",
    "def process_inputs_from_file(fileName): # Read in data\n",
    "    inputs = np.loadtxt(fileName, delimiter=' ')\n",
    "    X, _Y = inputs[:, :NUM_FEATURES], inputs[:, -1].astype(int)\n",
    "    X = scale(X, np.min(X, axis=0), np.max(X, axis=0))\n",
    "    _Y[_Y == 7] = 6 # Actually dont have, just in case have error data\n",
    "\n",
    "    Y = np.zeros((_Y.shape[0], NUM_CLASSES))\n",
    "    Y[np.arange(_Y.shape[0]), _Y - 1] = 1 #one hot matrix\n",
    "    return X, Y\n",
    "\n",
    "def init_weights(feature_no, neuron_no, name, logistic = True):\n",
    "    # From eg.5.2\n",
    "    n_in = feature_no\n",
    "    n_out = neuron_no\n",
    "    W_values = np.asarray(np.random.uniform(low=-np.sqrt(6. / (n_in + n_out)),\n",
    "                                            high=np.sqrt(6. / (n_in + n_out)),\n",
    "                                            size=(n_in, n_out)))\n",
    "    if logistic == True:\n",
    "        W_values *= 4\n",
    "    return(tf.Variable(W_values, dtype=tf.float32, name=name))\n",
    "\n",
    "def init_bias(neuron_no, name):\n",
    "    # From eg.5.2\n",
    "    return(tf.Variable(np.zeros(neuron_no), dtype=tf.float32, name=name))\n",
    "\n",
    "def setup_cross_entropy(labels, logits):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "\n",
    "def setup_correct_prediction(labels, logits):\n",
    "    return tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X_in, y_in, batch_size):\n",
    "    X = list(X_in)\n",
    "    y = list(y_in)\n",
    "    \n",
    "    if len(X)!=len(y):\n",
    "        print(\"Error: len(X)!=len(Y)\")\n",
    "        return None\n",
    "    batched_X = []\n",
    "    batched_y = []\n",
    "    count = 0\n",
    "    while (len(X) >= batch_size):\n",
    "        batched_X.append(X[0:batch_size])\n",
    "        del X[0:batch_size]\n",
    "        batched_y.append(y[0:batch_size])\n",
    "        del y[0:batch_size]\n",
    "        if count % 50 == 0:\n",
    "            print (count)\n",
    "        count += 1\n",
    "    print(count)\n",
    "    if len(X) != 0:\n",
    "        remain = batch_size-len(X)\n",
    "        X.extend(batched_X[0][0:remain])\n",
    "        y.extend(batched_y[0][0:remain])\n",
    "        batched_X.append(X)\n",
    "        batched_y.append(y)\n",
    "        print(\"Remain rescaled to\", len(X))\n",
    "    \n",
    "    return (batched_X, batched_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Values\n",
    "NUM_FEATURES = 36\n",
    "NUM_CLASSES = 6\n",
    "NUM_HIDDEN = 10\n",
    "\n",
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 5000\n",
    "BATCH_SIZE = 32\n",
    "NUM_NEURONS = 10\n",
    "SEED = 10\n",
    "BETA = pow(10, -6)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "TRAIN_FILE_NAME = 'sat_train.txt'\n",
    "TEST_FILE_NAME = 'sat_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, trainY = process_inputs_from_file(TRAIN_FILE_NAME)\n",
    "testX, testY = process_inputs_from_file(TEST_FILE_NAME)\n",
    "\n",
    "x = tf.placeholder(tf.float32, [None, NUM_FEATURES], name='x')\n",
    "d = tf.placeholder(tf.float32, [None, NUM_CLASSES], name='d')\n",
    "\n",
    "with tf.variable_scope(\"Hidden_layer\"):\n",
    "    W = init_weights(NUM_FEATURES, NUM_HIDDEN, name=\"Weight_1\")\n",
    "    b = init_bias(NUM_HIDDEN, name=\"Bias_1\")\n",
    "    z = tf.matmul(x, W) + b #syn_input_1\n",
    "    h = tf.nn.sigmoid(z) #out_1\n",
    "\n",
    "with tf.variable_scope(\"Output_layer\"):\n",
    "    V = init_weights(NUM_HIDDEN, NUM_CLASSES, name=\"Weight_2\")\n",
    "    c = init_bias(NUM_CLASSES, name=\"Bias_2\" )\n",
    "    u = tf.matmul(h, V) + c #syn_out_2\n",
    "    y = u #out_2\n",
    "\n",
    "cross_entropy = setup_cross_entropy(labels=d, logits=y)\n",
    "regularization = tf.nn.l2_loss(V) + tf.nn.l2_loss(W) \n",
    "J = tf.reduce_mean(cross_entropy + BETA * regularization)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n",
    "train_op = optimizer.minimize(J)\n",
    "\n",
    "correct_prediction = setup_correct_prediction(labels=d, logits=y)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /save/model-PartA-5000-32.ckpt\n",
      "Model restored.\n",
      "W : [[ 0.783916   -1.3846724   0.38614455  0.7188592  -0.00431363 -0.7951342\n",
      "  -0.872375    0.7527411  -0.956025   -1.1893935 ]\n",
      " [ 0.5355528   1.3099715  -1.4332227   0.03522662  0.90324336  0.3251171\n",
      "   0.64070886 -0.60132426  1.2070583   0.6199653 ]\n",
      " [ 0.12292176 -1.0338639  -0.36595154  0.5031174  -0.16805911 -0.19065076\n",
      "   0.34025946  0.03795981  0.43453664  0.29192784]\n",
      " [ 0.8818693   0.06254426  1.180693   -0.522274   -1.1832696  -0.57582945\n",
      "  -1.1152997   0.94964594 -1.3091346   0.36487648]\n",
      " [ 0.1374888   0.9225033  -0.869819    1.0310335  -0.42861417  0.7357435\n",
      "  -0.5895198   1.1092925  -0.50414234 -0.96785635]\n",
      " [-0.310511   -1.1745988   0.92775786 -1.0079126  -0.33482355  1.283585\n",
      "   1.408877   -0.12624754  0.9422538  -0.71834487]\n",
      " [ 0.28133205  1.1638858   0.09984691  0.26061523 -1.331135   -0.4126391\n",
      "  -1.2146072  -0.562077   -0.48909602  0.7911671 ]\n",
      " [-1.3291776  -0.20371546 -0.5347245   0.39435846 -0.4439431  -1.3201108\n",
      "   1.0976739   0.76057065  1.0924197  -0.23833758]\n",
      " [ 0.30504107  0.03890859  0.28267556 -0.6870209  -0.57533467 -1.3712436\n",
      "  -0.5690036  -0.74521     0.16635838  0.18926685]\n",
      " [-0.07184941 -0.5986606  -1.2589922   1.3834332  -0.4631258  -0.0143058\n",
      "   1.3784105  -0.17111985 -0.5250572   0.05719865]\n",
      " [ 0.2257565   1.0226068  -1.2478795  -0.10247973  0.8146244   0.63160044\n",
      "   0.24853991 -1.3374548  -0.4314926   0.18257435]\n",
      " [-0.5786326   0.03563658  0.50119114 -0.9846781  -1.2987871  -0.46859214\n",
      "  -1.1324059  -0.9277334   1.114755   -0.38899568]\n",
      " [-0.8125486   0.7295272  -1.1358273   0.7067225  -0.08729815  0.28388622\n",
      "  -1.0181171  -0.9129057   0.41915116 -1.3041313 ]\n",
      " [-0.72632396  0.12252925 -0.789423   -0.34263298  1.2199405   1.2289668\n",
      "   0.19285792  0.09670611 -1.4016957   1.3807755 ]\n",
      " [ 0.21099952  0.84296197  0.17785525  1.0902199   0.24326368  0.60342157\n",
      "  -1.0154784  -0.20672445  0.5601989  -1.1423565 ]\n",
      " [-0.1744962  -0.96442896  0.02016308  0.9217777  -1.1842884   0.8669768\n",
      "   0.188167    0.25814882 -0.8722658  -0.184571  ]\n",
      " [-0.58968717 -1.3361163  -1.3559737  -0.13549186  0.7074761   0.16554135\n",
      "  -0.33193675 -0.95902413  0.9773251   0.28618646]\n",
      " [ 0.8168368   1.0069338   0.2980647   0.8120573   0.33439398 -1.3834785\n",
      "   0.7236575  -0.9359987  -0.11986326  0.03791492]\n",
      " [-0.04616781  0.9950203  -0.93954736 -1.4023463   1.0076703   0.70115006\n",
      "  -0.1251121  -0.24010214 -1.1073682  -0.466098  ]\n",
      " [-1.1711357   0.6235916  -1.2219105  -0.8495863   0.21315876 -0.59567434\n",
      "   0.4499351   0.877088   -0.42988294 -1.1746566 ]\n",
      " [ 0.9052518   0.82305413 -0.30793995  1.0530753  -0.3350653  -0.70121515\n",
      "   0.9517279   0.6829712   0.021961    0.4169972 ]\n",
      " [-0.8286787   1.1433063   1.34624    -0.52873015  1.0561768  -0.5481398\n",
      "  -1.3716362  -1.3024926  -0.9111952  -1.2451749 ]\n",
      " [-0.7007167   1.1949453  -0.12178374 -1.0684144   0.8953577  -0.27890077\n",
      "  -1.374038    1.0309778  -0.65212107  0.6041616 ]\n",
      " [-0.41671127  0.85033554  0.9956937   0.11021812  0.17071633 -1.090667\n",
      "  -0.35352436 -0.20586753  0.0323847   1.1319041 ]\n",
      " [-0.57724357 -0.29920638  0.8473428  -0.25345725 -0.91049975  0.9830631\n",
      "   0.20014311 -1.2711296   1.4126085  -0.7836185 ]\n",
      " [ 0.33140314 -0.3813053   0.87479854  0.36855942  0.92807806 -0.61179185\n",
      "  -0.65067554  0.29196525  1.2669817   0.59893304]\n",
      " [-0.6835899  -1.09117    -0.11925645  1.3718569   0.5217264  -0.88626987\n",
      "  -1.3095884   1.3078063   1.0362304   0.9473977 ]\n",
      " [ 1.3403294   0.28585318  1.2450281  -1.020577   -1.1153692  -0.36974183\n",
      "   0.7701616  -0.51676726  0.3988193  -1.4179881 ]\n",
      " [ 0.12558182 -0.46876907  1.1499959   1.2733175  -0.33480215 -0.30014277\n",
      "   1.1478453  -1.274677   -0.2733839  -1.0685192 ]\n",
      " [-1.1933529   0.8204468  -0.5646385   0.2428593   1.0934874   1.1002074\n",
      "   0.291683    0.41960078 -1.2810166  -0.5851212 ]\n",
      " [ 0.5540868  -0.89847493  0.8754677   0.04265632  0.74336636 -0.93095225\n",
      "  -1.2059187  -0.05179875  0.08336637  0.56718576]\n",
      " [-0.8530206   0.4951393   0.84733146 -1.3240389   1.3387605   1.3735334\n",
      "   0.1463722  -1.257097   -0.4471512  -1.3856027 ]\n",
      " [ 0.8692393  -0.8438747  -1.0307364   0.57754576 -1.2771982  -0.7032241\n",
      "   0.02985122  1.4309317  -1.0213083  -0.14586496]\n",
      " [ 0.29308766 -1.1635847  -0.61039996  0.63794845  0.14679195  0.9782372\n",
      "   0.23209819 -0.9113544   0.3337157   1.1180141 ]\n",
      " [ 0.0485086   0.36446193  0.01461711  1.1836219  -0.25048542  0.10247836\n",
      "  -0.454645   -1.0702492   0.47043446  1.2587641 ]\n",
      " [ 0.32570398  0.9902608  -0.815036    1.1648681  -1.4164022  -0.7921985\n",
      "  -1.0630885   1.1774977   1.1852632   0.23419741]]\n",
      "b : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[[-0.589909   -1.2375445  -1.011406   -2.8379142  -2.142142    1.6380386 ]\n",
      " [-0.47425184 -1.2615708  -1.0729344  -2.9255736  -2.0671847   1.6435599 ]\n",
      " [-0.9474944  -1.092849   -0.80646724 -2.891448   -2.384553    1.9000216 ]\n",
      " ...\n",
      " [-0.6665058  -0.5869827  -0.68709624 -2.2435439  -2.1330605   2.1103666 ]\n",
      " [-0.19934294 -0.508572   -0.7805999  -2.018778   -2.123908    1.5590326 ]\n",
      " [-0.14042649 -0.4966476  -0.4786117  -1.8086927  -1.9822166   1.4936738 ]] \n",
      " 0.235\n"
     ]
    }
   ],
   "source": [
    "model_name = \"model-PartA-5000-32\"\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    # Restore variables from disk.\n",
    "    saver.restore(sess, \"/save/\"+model_name+\".ckpt\")\n",
    "    print(\"Model restored.\")\n",
    "    # Check the values of the variables\n",
    "    print(\"W : %s\" % W.eval())\n",
    "    print(\"b : %s\" % b.eval())\n",
    "    \n",
    "#     tf.global_variables_initializer().run()\n",
    "    output_2_, accuracy_ = sess.run([y, accuracy], feed_dict={x: testX, d: testY})\n",
    "    print(output_2_, '\\n',accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
