{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lzcai\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import learn\n",
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "import time\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.01\n",
    "EPOCHS = 10000\n",
    "BATCH_SIZE = 128\n",
    "MAX_DOC_LEN = 100\n",
    "CHAR_DEPTH = 256\n",
    "WORD_WIDTH = 20\n",
    "NUM_CLASSES = 15\n",
    "DROP = True\n",
    "\n",
    "seed = 10\n",
    "np.random.seed(seed)\n",
    "tf.set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"..\\data\"\n",
    "SAVE_DIR = \"..\\data\\PartB_Result\"\n",
    "RESULT_DIR = \"..\\data\\PartB_Result\\Q2\\1110\"\n",
    "TRAIN_CHAR_FILENAME = \"Train_char.out\"\n",
    "TEST_CHAR_FILENAME = \"Test_char.out\"\n",
    "TRAIN_WORD_FILENAME = \"Train_word_raw.out\"\n",
    "TEST_WORD_FILENAME = \"Test_word_raw.out\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_DIR, TRAIN_WORD_FILENAME),\"rb\") as f:\n",
    "    tmp_trainX, tmp_trainY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(SAVE_DIR, TEST_WORD_FILENAME),\"rb\") as f:\n",
    "    tmp_testX, tmp_testY = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Expand 100-dim vector to 100*20 Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_vector(tmp_X, slot_range = 2000):\n",
    "    vectorX = []\n",
    "    for tmp in tmp_X:\n",
    "        record = []\n",
    "        for item in tmp:\n",
    "            word = [0]*20\n",
    "            tmp_item = item\n",
    "            last_slot = int(item / slot_range)\n",
    "            rem_value = item % slot_range\n",
    "            for i in range(0, last_slot):\n",
    "                word[i] = 2000\n",
    "            word[last_slot] = rem_value\n",
    "            record.append(word)\n",
    "        vectorX.append(record)\n",
    "    return vectorX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_label = max([tmp.max() for tmp in [tmp_trainX, tmp_testX]])\n",
    "slot_range = int(max_label / WORD_WIDTH)+1\n",
    "slot_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = generate_vector(tmp_trainX, 2000)\n",
    "testX = generate_vector(tmp_testX, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(X_in, y_in, batch_size):\n",
    "    X = list(X_in)\n",
    "    y = list(y_in)\n",
    "    \n",
    "    if len(X)!=len(y):\n",
    "        print(\"Error: len(X)!=len(Y)\")\n",
    "        return None\n",
    "    batched_X = []\n",
    "    batched_y = []\n",
    "    count = 0\n",
    "    while (len(X) >= batch_size):\n",
    "        batched_X.append(X[0:batch_size])\n",
    "        del X[0:batch_size]\n",
    "        batched_y.append(y[0:batch_size])\n",
    "        del y[0:batch_size]\n",
    "        if count % 10 == 0:\n",
    "            print (count)\n",
    "        count += 1\n",
    "    \n",
    "    if len(X) != 0:\n",
    "        remain = batch_size-len(X)\n",
    "        X.extend(batched_X[0][0:remain])\n",
    "        y.extend(batched_y[0][0:remain])\n",
    "        batched_X.append(X)\n",
    "        batched_y.append(y)\n",
    "        print(count, \"Remain rescaled to\", len(X))\n",
    "    \n",
    "    return (batched_X, batched_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainY_targets = np.array(tmp_trainY).reshape(-1)\n",
    "trainY = np.eye(NUM_CLASSES)[trainY_targets]\n",
    "\n",
    "testY_targets = np.array(tmp_testY).reshape(-1)\n",
    "testY = np.eye(NUM_CLASSES)[testY_targets]\n",
    "# trainY = y_train\n",
    "# testY = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "43 Remain rescaled to 128\n"
     ]
    }
   ],
   "source": [
    "x_allBatch, y_allBatch = generate_batch(trainX, trainY, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 128, 100, 20)\n",
      "(44, 128, 15)\n"
     ]
    }
   ],
   "source": [
    "x_allBatch = np.array(x_allBatch)\n",
    "y_allBatch = np.array(y_allBatch)\n",
    "\n",
    "print(x_allBatch.shape)\n",
    "print(y_allBatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(num, data, labels):\n",
    "    '''\n",
    "    Return a total of `num` random samples and labels. \n",
    "    '''\n",
    "    idx = np.arange(0 , len(data))\n",
    "    np.random.shuffle(idx)\n",
    "    idx = idx[:num]\n",
    "    data_shuffle = [data[ i] for i in idx]\n",
    "    labels_shuffle = [labels[ i] for i in idx]\n",
    "\n",
    "    return np.asarray(data_shuffle), np.asarray(labels_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn(X):\n",
    "#     X = tf.reshape(X, [-1, MAX_DOC_LEN, 1])\n",
    "#     h = tf.layers.dense(X, WORD_WIDTH)\n",
    "#     print(h)\n",
    "    X = tf.cast(X, tf.float32)\n",
    "    X = tf.reshape(X, [-1, MAX_DOC_LEN, WORD_WIDTH, 1])\n",
    "    print(X)\n",
    "    \n",
    "    #Conv 1\n",
    "    conv1 = tf.layers.conv2d(\n",
    "      inputs = X,\n",
    "      filters = 10,\n",
    "      kernel_size = [20, 20],\n",
    "      padding = \"valid\",\n",
    "      activation = tf.nn.relu)\n",
    "\n",
    "    pool1 = tf.layers.max_pooling2d(\n",
    "        inputs = conv1, \n",
    "        pool_size = [4, 4], \n",
    "        padding = \"same\",\n",
    "        strides = 2)\n",
    "\n",
    "    #Conv 2\n",
    "    conv2 = tf.layers.conv2d(\n",
    "      inputs = pool1,\n",
    "      filters = 10,\n",
    "      kernel_size=[20, 1],\n",
    "      padding=\"valid\",\n",
    "      activation=tf.nn.relu)\n",
    "\n",
    "    pool2 = tf.layers.max_pooling2d(\n",
    "        inputs = conv2, \n",
    "        pool_size = [4, 4], \n",
    "        padding = \"same\",\n",
    "        strides = 2)    \n",
    "\n",
    "    #Softmax    \n",
    "\n",
    "    dim = pool2.get_shape()[1].value * pool2.get_shape()[2].value * pool2.get_shape()[3].value \n",
    "    pool2_flat = tf.reshape(pool2, [-1, dim])\n",
    "\n",
    "    W2 = tf.Variable(tf.truncated_normal([dim, NUM_CLASSES], stddev=1.0/np.sqrt(dim)), name='weights_3')\n",
    "    b2 = tf.Variable(tf.zeros([NUM_CLASSES]), name='biases_3')\n",
    "    logits = tf.matmul(pool2_flat, W2) + b2\n",
    "\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_cross_entropy(labels, logits):\n",
    "    return tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=logits)\n",
    "\n",
    "def setup_correct_prediction(labels, logits):\n",
    "    return tf.cast(tf.equal(tf.argmax(logits, 1), tf.argmax(labels, 1)), tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Reshape:0\", shape=(?, 100, 20, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.int32, [None, MAX_DOC_LEN, WORD_WIDTH])\n",
    "d = tf.placeholder(tf.float32, [None, NUM_CLASSES])\n",
    "y = cnn(x)\n",
    "\n",
    "cross_entropy = setup_cross_entropy(labels=d, logits=y)\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(LEARNING_RATE)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = setup_correct_prediction(labels=d, logits=y)\n",
    "accuracy = tf.reduce_mean(correct_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "if not os.path.exists(RESULT_DIR):\n",
    "    print(\"Not Exist\")\n",
    "    os.makedirs(RESULT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graph(EPOCHS, BATCH_SIZE, acc_record, fileName, isTrain, error = False):\n",
    "    if error:  \n",
    "        acc_record = [1-tmp for tmp in acc_record]\n",
    "        if isTrain:\n",
    "            yLabel = 'Train error'\n",
    "        else:\n",
    "            yLabel = 'Test error'\n",
    "    else:\n",
    "        if isTrain:\n",
    "            yLabel = 'Train accuracy'\n",
    "        else:\n",
    "            yLabel = 'Test accuracy'\n",
    "    plt.figure(1)\n",
    "    plt.plot(range(EPOCHS), acc_record)\n",
    "    plt.xlabel(str(EPOCHS) + ' iterations')\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.savefig(os.path.join(RESULT_DIR, fileName))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(testX, testY):\n",
    "    output_2_, accuracy_ = session.run([y, accuracy], feed_dict={x: testX, d: testY})\n",
    "    print(output_2_, '\\n',accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_backup = []\n",
    "test_acc_backup = []\n",
    "time_usage_backup = []\n",
    "total_time_backup = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH_SIZE= 128\n",
      "iter 0: Train accuracy 0.0546875 Test accuracy:  0.071428575 *\n",
      "Time usage: 0:00:13\n",
      "iter 100: Train accuracy 0.0625 Test accuracy:  0.071428575 \n",
      "Time usage: 0:01:32\n",
      "iter 200: Train accuracy 0.0703125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:02:51\n",
      "iter 300: Train accuracy 0.117188 Test accuracy:  0.071428575 \n",
      "Time usage: 0:04:11\n",
      "iter 400: Train accuracy 0.109375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:05:30\n",
      "iter 500: Train accuracy 0.0859375 Test accuracy:  0.07 \n",
      "Time usage: 0:06:49\n",
      "iter 600: Train accuracy 0.0859375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:08:08\n",
      "iter 700: Train accuracy 0.0625 Test accuracy:  0.071428575 \n",
      "Time usage: 0:09:26\n",
      "iter 800: Train accuracy 0.0703125 Test accuracy:  0.07 \n",
      "Time usage: 0:10:45\n",
      "iter 900: Train accuracy 0.046875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:12:04\n",
      "iter 1000: Train accuracy 0.078125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:13:22\n",
      "iter 1100: Train accuracy 0.0625 Test accuracy:  0.071428575 \n",
      "Time usage: 0:14:40\n",
      "iter 1200: Train accuracy 0.0703125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:15:58\n",
      "iter 1300: Train accuracy 0.09375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:17:17\n",
      "iter 1400: Train accuracy 0.0234375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:18:35\n",
      "iter 1500: Train accuracy 0.046875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:19:53\n",
      "iter 1600: Train accuracy 0.046875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:21:12\n",
      "iter 1700: Train accuracy 0.101562 Test accuracy:  0.071428575 \n",
      "Time usage: 0:22:30\n",
      "iter 1800: Train accuracy 0.046875 Test accuracy:  0.07 \n",
      "Time usage: 0:23:48\n",
      "iter 1900: Train accuracy 0.109375 Test accuracy:  0.07 \n",
      "Time usage: 0:25:06\n",
      "iter 2000: Train accuracy 0.046875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:26:24\n",
      "iter 2100: Train accuracy 0.078125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:27:43\n",
      "iter 2200: Train accuracy 0.09375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:29:01\n",
      "iter 2300: Train accuracy 0.0546875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:30:19\n",
      "iter 2400: Train accuracy 0.0625 Test accuracy:  0.071428575 \n",
      "Time usage: 0:31:37\n",
      "iter 2500: Train accuracy 0.0625 Test accuracy:  0.071428575 \n",
      "Time usage: 0:32:56\n",
      "iter 2600: Train accuracy 0.0703125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:34:14\n",
      "iter 2700: Train accuracy 0.0546875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:35:32\n",
      "iter 2800: Train accuracy 0.046875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:36:50\n",
      "iter 2900: Train accuracy 0.078125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:38:08\n",
      "iter 3000: Train accuracy 0.101562 Test accuracy:  0.07 \n",
      "Time usage: 0:39:27\n",
      "iter 3100: Train accuracy 0.109375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:40:45\n",
      "iter 3200: Train accuracy 0.0546875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:42:03\n",
      "iter 3300: Train accuracy 0.078125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:43:22\n",
      "iter 3400: Train accuracy 0.078125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:44:41\n",
      "iter 3500: Train accuracy 0.046875 Test accuracy:  0.071428575 \n",
      "Time usage: 0:45:59\n",
      "iter 3600: Train accuracy 0.109375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:47:17\n",
      "iter 3700: Train accuracy 0.0625 Test accuracy:  0.071428575 \n",
      "Time usage: 0:48:36\n",
      "iter 3800: Train accuracy 0.078125 Test accuracy:  0.071428575 \n",
      "Time usage: 0:49:54\n",
      "iter 3900: Train accuracy 0.09375 Test accuracy:  0.071428575 \n",
      "Time usage: 0:51:13\n",
      "iter 4000: Train accuracy 0.0625 Test accuracy:  0.071428575 \n",
      "Time usage: 0:52:31\n"
     ]
    }
   ],
   "source": [
    "print(\"BATCH_SIZE=\", BATCH_SIZE)\n",
    "total_iterations = 0\n",
    "train_acc = []\n",
    "start_time = time.time()\n",
    "train_acc_record = []\n",
    "test_acc_record = []\n",
    "epoch_time_record = []\n",
    "\n",
    "best_test_acc = 0.0\n",
    "last_improvement = 0\n",
    "improved_str = \"\"\n",
    "test_count = 0\n",
    "train_count = 0\n",
    "mul = int(len(trainX)/BATCH_SIZE)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for i in range(EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "#         for j in range(len(x_allBatch)):\n",
    "        for j in range(mul):\n",
    "#             feed_dict_train = {x: x_allBatch[j], d: y_allBatch[j]}\n",
    "            x_batch, d_batch = next_batch(BATCH_SIZE, trainX, trainY)\n",
    "            feed_dict_train = {x: x_batch, d: d_batch}\n",
    "            sess.run(train_op, feed_dict=feed_dict_train)\n",
    "            train_acc_record.append(accuracy.eval(feed_dict=feed_dict_train))\n",
    "            train_count += 1\n",
    "            \n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time_diff = epoch_end_time-epoch_start_time\n",
    "        epoch_time_record.append(epoch_time_diff)\n",
    "\n",
    "        if (i < 100 and i % 10==0) or (i % 100 == 0) or (i == (EPOCHS - 1)):\n",
    "            test_count += 1\n",
    "            test_accuracy = sess.run(accuracy, feed_dict={x: testX, d: testY})\n",
    "            test_acc_record.append(test_accuracy)\n",
    "            if DROP:\n",
    "                if test_accuracy > best_test_acc:\n",
    "                    best_test_acc = test_accuracy\n",
    "                    last_improvement = i\n",
    "                    saver.save(sess=sess, save_path=RESULT_DIR)\n",
    "                    improved_str = \"*\"\n",
    "                else:\n",
    "                    improved_str = ''\n",
    "            else:\n",
    "                saver.save(sess=sess, save_path=RESULT_DIR)\n",
    "\n",
    "            print('iter %d: Train accuracy %g'%(i, train_acc_record[i]), 'Test accuracy: ',test_accuracy, improved_str)\n",
    "            print(\"Time usage: \" + str(timedelta(seconds=int(round(time.time()-start_time)))))\n",
    "\n",
    "\n",
    "# Ending time.\n",
    "end_time = time.time()\n",
    "\n",
    "# Difference between start and end-times.\n",
    "time_dif = end_time - start_time\n",
    "\n",
    "# Print the time-usage.\n",
    "print(\"Time usage: \" + str(timedelta(seconds=int(round(time_dif)))))\n",
    "\n",
    "train_graphName = \"PartB-Q2-Train\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".png\"\n",
    "test_graphName = \"PartB-Q2-Test\"+str(EPOCHS)+'-'+str(BATCH_SIZE)+\".png\"\n",
    "plot_graph(train_count, BATCH_SIZE, train_acc_record,train_graphName, isTrain = True, error=True)\n",
    "plot_graph(test_count, BATCH_SIZE, test_acc_record, test_graphName, isTrain = False)\n",
    "\n",
    "\n",
    "train_acc_backup.append(train_acc_record)\n",
    "test_acc_backup.append(test_acc_record)\n",
    "time_usage_backup.append(epoch_time_record)\n",
    "total_time_backup.append(time_dif)\n",
    "\n",
    "#=========== Save all the data for EACH TRAINING Has Done ============#\n",
    "fileNameTail = str(EPOCHS)+'-'+str(BATCH_SIZE)+\".out\"\n",
    "\n",
    "train_acc_filename = \"PartB-Q2-Train_Acc-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, train_acc_filename), 'wb') as fp:\n",
    "    pickle.dump(train_acc_backup, fp)\n",
    "\n",
    "test_acc_filename = \"PartB-Q2-Test_Acc-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, train_acc_filename), 'wb') as fp:\n",
    "    pickle.dump(test_acc_backup, fp)\n",
    "\n",
    "time_usage_filename = \"PartB-Q2-Time_Usage-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, time_usage_filename), 'wb') as fp:\n",
    "    pickle.dump(time_usage_backup, fp)\n",
    "\n",
    "time_usage_filename = \"PartB-Q2-Time_Usage-\"+fileNameTail\n",
    "with open(os.path.join(RESULT_DIR, time_usage_filename), 'wb') as fp:\n",
    "    pickle.dump(time_usage_backup, fp)        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_allBatch[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
